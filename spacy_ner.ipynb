{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - NER with Spacy\n",
    "In this second part of the challenge, we will be using the preprocessed data from part one to start training NER models. We will be using spacy (https://spacy.io/) here to \"get our feet wet\" with NER, as training spacy can be reasonably done on our laptops and does not yet necessarily require a GPU. Spacy is a powerful, effective, and resource-efficient NLP library - It might surprise us with its performance on the challenge!\n",
    "\n",
    "We will run spacy's pretrained models on our data to get a feel for NER, and then we will perform some additional preprocessing on our data before we start training our own NER model using the labelled entities we have identified in part one. \n",
    "We will also explore evaluation metrics for NER, and decide how we want to quantify the performance of our trained models. \n",
    "\n",
    "* *If you need help setting up python or running this notebook, please get help from the  assistants to the professor*\n",
    "* *It might be helpful to try your code out first in a python ide like pycharm before copying it an running it here in this notebook*\n",
    "* *For solving the programming tasks, use the python reference linked here (Help->Python Reference) as well as Web-searches.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reload preprocessed data\n",
    "Here, we will load the data we saved in part one and save it to a variable. Provide code below to load the data and store it as a list in a variable. (Hint - use 'open' and the json module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "690\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## import json module\n",
    "import json\n",
    "path = \"data/converted_resumes.json\"\n",
    "## TODO open file load as json and store in \"resumes\" variable\n",
    "with open(path) as f:\n",
    "    resumes = json.load(f)\n",
    "## TODO print length of loaded resumes list to be sure everything ok\n",
    "print(len(resumes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take Spacy for a spin\n",
    "Before we train our own NER model to recognize the resume-specific entities we want to capture, let's see how spacy's pretrained NER models do on our data. These pretrained models can't recognize our entities yet, but let's see how they do. Run the next code block to load spacy's English language model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<spacy.lang.en.English object at 0x7f68f4063588>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "print(nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the EntityRecognizer in the loaded nlp pipeline and display the labels it supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "('LANGUAGE', 'ORG', 'WORK_OF_ART', 'LAW', 'PERSON', 'DATE', 'CARDINAL', 'FAC', 'EVENT', 'ORDINAL', 'PERCENT', 'NORP', 'QUANTITY', 'LOC', 'PRODUCT', 'MONEY', 'GPE', 'TIME')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ner = nlp.get_pipe('ner')\n",
    "labels = ner.labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What do the 'GPE', 'FAC' and 'NORP' labels stand for? (Tipp: use either the spacy.explain method, or google the spacy.io api docs) \n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "* GPE: Countries, cities, states.\n",
    "* FAC: Buildings, airports, highways, bridges, etc.\n",
    "* NORP: Nationalities or religious or political groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "GPE:  Countries, cities, states\nFAC:  Buildings, airports, highways, bridges, etc.\nNORP:  Nationalities or religious or political groups\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "### TODO  if you choose to use spacy's 'explain' method to get the answer to the question above, provide your code here\n",
    "## print description of entities using spacy explain\n",
    "print(\"GPE: \", spacy.explain(\"GPE\"))\n",
    "print(\"FAC: \", spacy.explain(\"FAC\"))\n",
    "print(\"NORP: \", spacy.explain(\"NORP\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the entities are different than the entities we will train our custom model on. \n",
    "##### Question: what entities do you think this model will find in an example resume?\n",
    "PERSON, ORG, GPE/LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will work with one of our resumes, and get spacy to tell us what entities it recognizes. Complete the code block below to get a single resume text out of our resume list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Navas Koya\nTest Engineer\nMangalore, Karnataka - Email me on Indeed: indeed.com/r/Navas-Koya/23c1e4e94779b465\nWilling to relocate to: Mangalore, Karnataka - Bangalore, Karnataka - Chennai, Tamil Nadu\nWORK EXPERIENCE\nSystem Engineer\nInfosys -\nAugust 2014 to Present\n.NET application Maintenance and do the code changes if required\nTest Engineer\nInfosys -\nJune 2015 to February 2016\nPrProject 2:\nTitle: RBS W&G Proving testing.\nTechnology: Manual testing\nRole: Software Test Engineer\nDomain: Banking\nDescription:\nWrite test cases & descriptions. Review the entries. Upload and map the documents into\nHP QC. Execute the testing operations in TPROD mainframe. Upload the result in QC along with\nthe proof.\nRoles and Responsibilities:\n•Prepared the Test Scenarios\n•Prepared and Executed Test Cases\n•Performed functional, Regression testing, Sanity testing.\n•Reviewed the Test Reports and Preparing Test Summary Report.\n•Upload Test cases to the QC.\n•Execute in TPROD Mainframe.\n•Defect Track and Report.\nTest Executive\nInfosys Limited -\nAugust 2014 to May 2015\nhttps://www.indeed.com/r/Navas-Koya/23c1e4e94779b465?isid=rex-download&ikw=download-top&co=IN\n\nProject 1:\nTitle: CAWP (Compliance Automated Work Paper)\nTechnology: Manual testing\nRole: Software Test Executive\nDomain: Banking\nDescription:\nThe Admin can create and maintain annual test plan, and users can only view and add\ndetails. Testers will get Business Requirement which explains the flows and Functional\nrequirements which gives the full detail of the project.\nRoles and Responsibilities:\n•Prepared the Test Scenarios\n•Prepared and Executed Test Cases\n•Performed functional, Regression testing, Sanity testing.\n•Reviewed the Test Reports and Preparing Test Summary Report.\n•Defect Track and Report.\nEDUCATION\nBachelor of Computer Applications\nMangalore University, Mangalore\nJune 2011 to April 2014\nSKILLS\nC# (Less than 1 year), .NET, SQL Server, Css, Html5\nADDITIONAL INFORMATION\nBachelor of computer application: with 74% from Milagres College, Kallianpur under\nMangalore University, Karnataka.\nNavas Najeer Koya 2\nSKILL SET • ASP.NET, C# • QA tools\n• Coding and modularization • Excellent communication skills\n• VB, VB.net, ASP • Technical specifications creation\n• HTML • System backups\n• Sql server 2005, Oracle • System upgrades\n• Java/C/C++ • Excellent problem-solving abilities\nNavas Najeer Koya 3\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "### TODO get a single resume text and print it out\n",
    "restxt = resumes[42][0]\n",
    "## print it out, removing extraneous spaces\n",
    "print(\"\\n\".join(restxt.split('\\n\\n')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting entities with spacy is easy with a pretrained model. We simply call the model (here 'nlp') with our text to get a spacy Document. See https://spacy.io/api/doc for more detail. \n",
    "\n",
    "Execute the code below to process the resume txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(restxt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc object has a list of entities predicted by spacy 'ents'. We would like to loop through all of these entities and print their label and associated text to see what spacy predicted for this resume.\n",
    "\n",
    "Complete the code below to do this. You will probably need to google the spacy api docs to find the solution (Tipp: look for 'Doc.ents'). Also, trying code in your ide (for example pycharm) before copying it here might help with exploring and debugging to find the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Navas Koya  :  PERSON\nTest Engineer  :  PERSON\nKarnataka - Email  :  PERSON\nKarnataka - Bangalore  :  PERSON\nKarnataka - Chennai  :  ORG\nAugust 2014  :  DATE\nMaintenance  :  PERSON\nTest Engineer  :  PERSON\nJune 2015 to February 2016  :  DATE\nRBS W&G Proving  :  ORG\nHP QC  :  ORG\nTPROD  :  ORG\nResponsibilities  :  PERSON\nExecuted Test Cases  :  ORG\nthe Test Reports  :  ORG\nPreparing Test Summary Report  :  ORG\nTrack and Report  :  ORG\nInfosys Limited -\n\n  :  PERSON\nAugust 2014  :  DATE\nMay 2015  :  DATE\n1  :  CARDINAL\nCAWP  :  PERSON\nRole  :  PERSON\nSoftware Test Executive  :  ORG\nAdmin  :  ORG\nannual  :  DATE\nBusiness Requirement  :  ORG\nFunctional  :  ORG\nResponsibilities  :  PERSON\nthe Test Scenarios\n  :  ORG\nExecuted Test Cases  :  ORG\nthe Test Reports  :  ORG\nPreparing Test Summary Report  :  ORG\nTrack and Report  :  ORG\nComputer Applications\n\nMangalore University  :  ORG\nJune 2011  :  DATE\nApril 2014  :  DATE\nSQL Server, Css  :  ORG\n74%  :  PERCENT\nMilagres College  :  ORG\nKallianpur  :  GPE\nMangalore University  :  ORG\nKarnataka  :  GPE\nASP.NET  :  PERSON\n#  :  CARDINAL\nQA  :  ORG\nVB  :  ORG\nASP  :  ORG\nHTML  :  ORG\nSystem  :  PERSON\n2005  :  DATE\nOracle  :  ORG\nSystem  :  PRODUCT\nJava/C/C++  :  ORG\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##TODO loop through the doc's entities, and print the label and text for each entity found. \n",
    "for ent in doc.ents:\n",
    "    print(ent, \" : \", ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Questions: What is your first impression of spacy's NER based on the results above? Does it seem accurate/powerfull?\n",
    "The spacy's NER is not quite accurate.\n",
    "\n",
    "##### Does it make many mistakes? Do some entity types seem more accurate than others?\n",
    "Yes, there are some errors. The DATE entity is very accurate, the ORG, PRODUCT and PERSON are not.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as a comparison, we will list the entities contained in the resume's original annotated training data (remember, the existing annotations were created by a human-annotator, and not predicted by a machine like the entities predicted above) \n",
    "\n",
    "Complete the code below to do the following: \n",
    "* Access the 'entities' list of the example resume you chose, loop through the entities and print them out. \n",
    "* *Tip: one entity in the list is a tuple with the following structure: (12,1222,\"label\") where the first element is the start index of the entity in the resume text, the second element is the end index, and the third element is the label.\n",
    "* Use this Tip to print out a formatted list of entities \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Skills SKILL SET • ASP.NET, C# • QA tools\n\n• Coding and modularization • Excellent communication skills\n\n• VB, VB.net, ASP • Technical specifications creation\n\n• HTML • System backups\n\n• Sql server 2005, Oracle • System upgrades\n\n• Java/C/C++ • Excellent problem-solving abilities\n\nNavas Najeer Koya 3\n\nLocation Mangalore\n\nSkills C# (Less than 1 year), .NET, SQL Server, Css, Html5\n\n\nGraduation Year  2014\n\nLocation Mangalore\n\nLocation Mangalore\n\nDegree Bachelor of Computer Application\n\nGraduation Year  2014\n\nCompanies worked at Infosys\n\nDesignation Test Engineer\n\n\nCompanies worked at Infosys\n\nDesignation Test Engineer\n\n\nGraduation Year  2014\n\nCompanies worked at Infosys\n\nDesignation System Engineer\n\nLocation Mangalore\n\nLocation Mangalore\n\nDesignation Test Engineer\n\n\nName Navas Koya\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##TODO print original entities for one resume\n",
    "res = resumes[42]\n",
    "restext = res[0]\n",
    "labeled_ents = res[1]['entities']\n",
    "## TDOD print out formatted list of entity labels and text\n",
    "for ent in labeled_ents:\n",
    "    print(\"{} {}\\n\".format(ent[2], restext[ent[0]:ent[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, the annotated entities in the training data are different than the entities spacy can recognize with it's pretrainied models, so we need to train a custom NER model. We will get started with that now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Training Data for NER model training\n",
    "We need to do some more preprocessing of our training data before we can train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the entity labels you chose in part 1 of the challenge? We will be training a model to predict those entities.\n",
    "As a first step, we will gather all resumes that contain at least one training annotation for those entities.\n",
    "\n",
    "Complete and execute the code below to gather your training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Gathered 503 training examples\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##TODO Store the entity labels you want to train for as array in chosen_entity_labels\n",
    "chosen_entity_labels = [\"Skills\", \"Name\", \"Location\"]\n",
    "\n",
    "## this method gathers all resumes which have all of the chosen entites above.\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "## TODO use the gather candidates methods and store result in training_data variable\n",
    "training_data = gather_candidates(resumes, chosen_entity_labels)\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have those training examples which contain the entities we are interested in. Do you have at least a few hundred examples? If not, you might need to re-think the entities you chose or try just one or two of them and re-run the notebooks. It is important that we have several hundred examples for training (e.g. more than 200. 3-500 is better). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove other entity annotations from training data\n",
    "Now that we have our training data, we want to remove all but relevant (chosen) entity annotations from this data, so that the model we train will only train for our entities. Complete and execute the code below to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[\"Nazish Alam\\nConsultant - SAP ABAP\\n\\nGhaziabad, Uttar Pradesh - Email me on Indeed: indeed.com/r/Nazish-Alam/\\nb06dbac9d6236221\\n\\nWilling to relocate to: Delhi, Delhi - Noida, Uttar Pradesh - Lucknow, Uttar Pradesh\\n\\nWORK EXPERIENCE\\n\\nConsultant\\n\\nSAP ABAP -  Noida, Uttar Pradesh -\\n\\nNovember 2016 to Present\\n\\nCredence Systems, Noida\\n\\nCredence Systems is IT Infrastructure Management Company, offers end-to-end solutions.\\nCombining deep domain expertise with new technologies and a cost effective on-site/ offshore\\nmodel. Helping companies integrate key business processes, improving their operational\\nefficiencies and extracting, better business value from their investment.\\n\\nPROJECT UNDERTAKEN\\nClient ECC Version Role and Responsibilities\\nWelspun Group Plate & Coil Mills Division\\nSAP ECC 6.0\\n\\nConsultant\\n\\nSAP ABAP -\\n\\nJanuary 2016 to Present\\n\\nReports:\\n• Designed technical program specifications based on business requirements.\\n• Generated basic lists and Interactive Reports for information in the MM/SD including Sales,\\nBilling, Purchasing, Goods Received, Inspection Plan, and Batch Determination using ABAP\\nprograms, Screen, Report Painter and Menu Painters. Used Parameters, Select-options and Match\\nCodes to make the reports more friendly and intuitive to the user.\\n• Generated different kind of reports like for PR (Purchase Requisition) analysis using ALV, PO\\n(Purchase Order) Pricing details, Pending Export Sales order etc.\\n• Developed report for the daily production done.\\nSAP Scripts:\\n• Generated various client specific Layout sets and form letters using SAP Script.\\n• Involved in modification of SAP scripts for Purchase orders (MEDRUCK) and indents, Delivery\\nnotes (RVDELNOTE), and Invoices (RVINVOICE) according to customer needs.\\n• Modified existing layout sets for Purchase Order and GR using SAP Script.\\nData Migration:\\n\\nhttps://www.indeed.com/r/Nazish-Alam/b06dbac9d6236221?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Nazish-Alam/b06dbac9d6236221?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Implemented both Call Transaction and Session Method of BDC accordingly, depending upon\\nthe size, type, state and created routines for data upload using data extracts for sequential files\\non the application server and UPLOAD/WS_UPLOAD for local files on the presentation server.\\n• Wrote ABAP programs for extracting data from SAP tables (Vendor master, Purchase Orders,\\nInvoices and remittance) to be transferred to vendors using non-SAP systems for reconciliation\\nand their local use.\\nObject Oriented:\\n• Created local and global classes with SE24 and within programs.\\n• Used the Standard ALV classes in OOPs ALV reports.\\n• Used ABSTRACT classes and Interfaces.\\n• Having knowledge and used the different object oriented concepts technically.\\n\\nSKILLS\\n\\nSAP (2 years), ABAP (2 years), ADBC (Less than 1 year), C++ (Less than 1 year), DATA\\nMODELING (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nOTHER TECHNICAL SKILLS\\n• Trained on SAP S4 HANA.\\n• Having knowledge of Code Push down, CDS view and it's consumption in ABAP.\\n• Data Modeling, creation of different type of views.\\n• AMDP.\\n• ADBC connectivity.\\n• Familiar with SQL, DDL, DML syntaxes.\\n• Work on Windows 7, Windows XP, Windows 8, Windows 10 OS, can work on C, C++\\nACADEMEIC CREDENTIALS\\n2015 Master of Computer Application\\nUPTU. India\",\n {'entities': [[2941, 3244, 'Skills'],\n   [2780, 2892, 'Skills'],\n   [35, 44, 'Location'],\n   [0, 11, 'Name']]}]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "## filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "## TODO use method above to remove all but relevant (chosen) entity annotations and store in X variable \n",
    "X = [[res[0], dict(entities=filter_ents(res[1]['entities'], chosen_entity_labels))] for res in training_data]\n",
    "X[42]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove resumes that cause errors in spacy\n",
    "Depending on what entities you chose, some of the resumes might cause errors in spacy. We don't need to get into details as to why, suffice to say it has to do with whitespace and syntax in the entity annotations. If these resumes are not removed from our training data, spacy will throw an exception during training, so we need to remove them first. \n",
    "\n",
    "We will use the remove_bad_data function below to do this. This function does the following:\n",
    "* calls train_spacy_ner with debug=True and n_iter=1. This causes spacy to process the documents one-by-one, and gather the documents that throw an exception in a list of \"bad docs\" which it returns. \n",
    "* You will complete the function to remove any baddocs (returned by remove_bad_data) from your training data list. \n",
    "\n",
    "You may or may not have any bad documents depending on the entities you chose. In any case, there should not be more than a dozen or so bad docs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Created blank 'en' model\n",
      "Exception thrown when processing doc:\n(\"Neeraj Dwivedi\\nSenior Sales Executive - Kansai Nerolac Paints Ltd\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Neeraj-Dwivedi/8f053ed44cdef8b2\\n\\nWORK EXPERIENCE\\n\\nSenior Sales Executive\\n\\nKansai Nerolac Paints Ltd -  Mumbai, Maharashtra -\\n\\nAugust 2017 to Present\\n\\nIncreased the sales from -22% to +6% through existing dealers and prospecting new dealers to\\nachieve the target within 6 months of joining.\\n• Maintaining excellent relations with dealers to increase revenue by 10%.\\n• Management of sales team (up to 3 members)\\n• Achieved monthly targets amounting to 40 - 60 lakhs monthly with timely collection.\\n• Maintain accounts clarity with dealers and ensure that the credit notes reach the dealers on\\ntime.\\n• Business development by conducting regular meetings with influencers such as architects,\\nbuilding contractors, housing societies, painters.\\n• Ensuring proper visibility of the products at various counters while also managing field\\ndevelopment using new sign boards and painter meets.\\n• Provide Briefing to the dealers about the new schemes and target products.\\n• Handling depot and ensuring the proper service and support to the market and at the same\\ntime handling the depot team of 14-15 members.\\n• Handled the entire western and south market of Mumbai ( From Churchgate to Dahanu)\\n• Presently handling two territories Vasai and Virar ( Mira Road To Dahanu)\\n• Developed the Bhayandar and Mira Road market which was almost dead for the company.\\n\\nSenior Sales Executive\\n\\nGreenPly Industries Ltd -  Mumbai, Maharashtra -\\n\\nAugust 2015 to August 2017\\n\\nBuilding the business within the territory using a variety of sales techniques.\\n• Targeting potential dealers and distributors and assessing opportunities for sales.\\n• Arranging potential dealers meetings and selling products offering.\\n• Team handling and Achieving Primary and secondary sales figures as well individually and along\\nwith team.\\n• Create and execute a territory sales and channel development plan that meets or exceeds\\nestablished sales quotas and supports Company revenue and profit targets.\\n• Complete sales activity reports and presentations in a timely manner. Appointed 150 dealers\\nand 3 distributors for wall-coverings imported by Greenply.\\n\\nArea Sales Manager\\n\\nGloob Décor Interior Design Pvt. Ltd -  Pune, Maharashtra -\\n\\nhttps://www.indeed.com/r/Neeraj-Dwivedi/8f053ed44cdef8b2?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nJuly 2014 to July 2015\\n\\nMumbai, Rest of Maharashtra (Pune, Nashik, Kolhapur and more), Goa, Jaipur\\nArea Sales Manager (July 2014 - July 2015)\\n\\n• Planning and scheduling individual/ team assignments to achieve the pre set goals within\\ntime, quality and cost parameters. Formulating long term/short term strategic plans to enhance\\noperations.\\n• Tracking market/ competitor trends to keep a track regarding changing client's requirement/\\nexpectations.\\n• Dealer/ distributor channel visiting, handling channel sales team (up to 6 members) across\\nMaharashtra, Mumbai and Goa.\\n• Business development and Team Management.\\n• Developed the market for Pune, Kolhapur, Jaipur and appointed 2 franchisee stores and 3\\ndistributors (35-40 lakhs investment)\\n\\nInternship\\n\\nMumbai, Maharashtra -\\n\\nFebruary 2014 to May 2014\\n\\nProvided career counseling and vocational training to the students.\\n\\nProject Management\\n\\nAARK alliance -  Delhi, Delhi -\\n\\nMay 2013 to July 2013\\n\\nProject: Sales and Customer service of construction equipments\\n\\nEDUCATION\\n\\nMMS\\n\\nMaratha Mandir Babasaheb Gawde Institute of Management studies, mumbai\\n\\nSKILLS\\n\\nSALES (4 years), SALES AND (2 years), TEAM MANAGEMENT (1 year), AND MARKETING (Less\\nthan 1 year), BUSINESS MANAGEMENT (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nCore Skills:\\n\\n• 4 years of experience in corporate sales and marketing.\\n• Planing, strategizing and implementing business growth, requirement analysis and technical\\nguidance for clients\\n• Interpersonal skills, business management, negotiation skills, team management, lead\\ngeneration, loyalty programs management\\n\\n\\n\\nSoft Skills:\\n\\n• SAP, CRM, MS word, MS excel, MS power point, Tally, ERP, Social Media.\",) ({'entities': [[4018, 4089, 'Skills'], [4016, 4017, 'Name'], [4015, 4016, 'Name'], [4002, 4003, 'Name'], [4001, 4002, 'Name'], [4000, 4001, 'Name'], [3999, 4000, 'Name'], [3959, 3960, 'Name'], [3872, 3873, 'Name'], [3851, 3852, 'Name'], [3758, 3759, 'Name'], [3702, 3999, 'Skills'], [3700, 3701, 'Name'], [3699, 3700, 'Name'], [3686, 3687, 'Name'], [3685, 3686, 'Name'], [3662, 3663, 'Name'], [3661, 3662, 'Name'], [3608, 3609, 'Name'], [3525, 3661, 'Skills'], [3524, 3525, 'Name'], [3523, 3524, 'Name'], [3516, 3517, 'Name'], [3515, 3516, 'Name'], [3444, 3445, 'Name'], [3443, 3444, 'Name'], [3439, 3440, 'Name'], [3438, 3439, 'Name'], [3428, 3429, 'Name'], [3427, 3428, 'Name'], [3364, 3365, 'Name'], [3363, 3364, 'Name'], [3341, 3342, 'Name'], [3340, 3341, 'Name'], [3308, 3309, 'Name'], [3307, 3308, 'Name'], [3288, 3289, 'Name'], [3287, 3288, 'Name'], [3219, 3220, 'Name'], [3218, 3219, 'Name'], [3192, 3193, 'Name'], [3191, 3192, 'Name'], [3170, 3176, 'Location'], [3169, 3170, 'Name'], [3168, 3169, 'Name'], [3157, 3158, 'Name'], [3156, 3157, 'Name'], [3118, 3119, 'Name'], [3028, 3029, 'Name'], [2984, 2985, 'Name'], [2969, 2975, 'Location'], [2955, 2956, 'Name'], [2862, 2863, 'Name'], [2848, 2849, 'Name'], [2754, 2755, 'Name'], [2742, 2743, 'Name'], [2647, 2648, 'Name'], [2556, 2557, 'Name'], [2555, 2556, 'Name'], [2512, 2513, 'Name'], [2438, 2444, 'Location'], [2437, 2438, 'Name'], [2436, 2437, 'Name'], [2413, 2414, 'Name'], [2412, 2413, 'Name'], [2411, 2412, 'Name'], [2313, 2314, 'Name'], [2312, 2313, 'Name'], [2252, 2253, 'Name'], [2251, 2252, 'Name'], [2232, 2233, 'Name'], [2231, 2232, 'Name'], [2171, 2172, 'Name'], [2077, 2078, 'Name'], [2003, 2004, 'Name'], [1913, 1914, 'Name'], [1902, 1903, 'Name'], [1805, 1806, 'Name'], [1735, 1736, 'Name'], [1649, 1650, 'Name'], [1569, 1570, 'Name'], [1568, 1569, 'Name'], [1541, 1542, 'Name'], [1540, 1541, 'Name'], [1519, 1525, 'Location'], [1491, 1492, 'Name'], [1490, 1491, 'Name'], [1467, 1468, 'Name'], [1466, 1467, 'Name'], [1380, 1381, 'Name'], [1304, 1305, 'Name'], [1269, 1275, 'Location'], [1219, 1220, 'Name'], [1172, 1173, 'Name'], [1081, 1082, 'Name'], [1004, 1005, 'Name'], [951, 952, 'Name'], [860, 861, 'Name'], [809, 810, 'Name'], [718, 719, 'Name'], [712, 713, 'Name'], [617, 618, 'Name'], [531, 532, 'Name'], [486, 487, 'Name'], [411, 412, 'Name'], [364, 365, 'Name'], [271, 272, 'Name'], [270, 271, 'Name'], [247, 248, 'Name'], [246, 247, 'Name'], [225, 231, 'Location'], [195, 196, 'Name'], [194, 195, 'Name'], [171, 172, 'Name'], [170, 171, 'Name'], [154, 155, 'Name'], [153, 154, 'Name'], [67, 73, 'Location'], [66, 67, 'Name'], [65, 66, 'Name'], [14, 15, 'Name'], [0, 14, 'Name']]},)\n",
      "Exception thrown when processing doc:\n(\"Jitendra Razdan\\nBusiness Development - Shapoor ji Pallonji Group Company\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Jitendra-Razdan/66b1e69aff1842bd\\n\\n• Dynamic professional with more than 7 years of work experience in Sales and Customer\\nService to Banking, Government & Corporate Sector.\\n• Proficient in managing day to day sales activities, marketing and client servicing activities.\\n• Ability to develop and maintain relationship with key decision makers and resolving\\ncritical problems.\\n• Ability to think innovatively and grasp key concepts quickly..\\n• Effective communicator with an ability to deal effectively with various departments\\n\\nWORK EXPERIENCE\\n\\nBusiness Development\\n\\nShapoor ji Pallonji Group Company -  Mumbai, Maharashtra -\\n\\nJuly 2011 to Present\\n\\n2011.\\n\\n(Areas: Mumbai, Delhi, NCR, Punjab, Himachal Pradesh, Haryana and J&K)\\n\\nArea of Expertise & Exposure\\nKey Account Management:\\n• Interface with individuals / key influencers for ascertaining requirements, making Presentations\\nand delivering need based product solutions.\\n• Ensure speedy resolution of queries & grievances to maximize client satisfaction levels.\\n• Maintain excellent relations with customers to generate avenues for further business.\\n\\nBusiness Development:\\n• Analyze business potential, conceptualize & execute strategies to drive sales, augment\\nturnover and achieve desired targets.\\n• Monitor competitor activities and devise effective counter measures.\\n• Identify, explore and develop new markets and tap profitable business opportunities.\\n\\nKEY RESPONSIBILITIES\\n• Playing an integral role in new business pitches and hold responsibility for the effective on-\\nboarding of new clients.\\n• Responsible for the development and achievement of sales through the direct sales.\\n• Focusing on growing and developing existing clients, together with generating new business.\\n• Write business plans for all current and opportunity tender business.\\n• Act as the key interface between the customer and all relevant divisions.\\n• Preparation of technical proposals and participation in business development meetings with\\nexisting and new clients.\\n• Develop and expand client relationships with existing clients in order to generate repeat\\nbusiness.\\n\\nhttps://www.indeed.com/r/Jitendra-Razdan/66b1e69aff1842bd?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Negotiate projects with clients, issue proposals and quotations, follow-up quotations.\\n• Looks after customer satisfaction regarding operational issues.\\n• Giving sales presentations to high-level executives.\\n• Managing employer's client relationships and providing clients with excellent service and\\nsupport.\\n• Feeding back all suggestions for improvement and market research to senior staff.\\n• Market Research\\n• People and Performance Management\\n• Pre and Post Sales\\n\\nKey Accounts Manager\\n\\nForbes Technosys Limited -  Mumbai, Maharashtra\\n\\na\\n\\nSales Executive\\n\\nSUN SYSTEM PVT LIMITED -  Delhi, Delhi -\\n\\nJanuary 2011 to May 2011\\n\\nwith Sun System Pvt Limited, sole distributor for APPLE\\nproducts as well as computer related IT accessories, at Delhi\\nDuration: Jan 2011 to May 2011\\n\\nEDUCATION\\n\\nB.E in Electronics & Telecommunication\\n\\nS S JONDHALE College Of Engineering -  Mumbai, Maharashtra\\n\\nSKILLS\\n\\nCONFIDENT (Less than 1 year), MS OFFICE (Less than 1 year), PROACTIVE (Less than 1 year),\\nREPORTING TOOLS (Less than 1 year), SELF MOTIVATED (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nSKILLS/STRENGTHS:\\n\\n• Good Communication Skills\\n• Good Analytical and Research Skills\\n• Well organized\\n• Multi tasking\\n• Team player\\n• Determined\\n• Confident\\n• Innovative\\n• Proactive\\n\\n\\n\\nTECHNICAL ATTRIBUTES:\\n\\n• Operating Systems: Windows XP or higher\\n• Reporting Tools: Explorer, MS Office\",) ({'entities': [[3436, 3598, 'Skills'], [3358, 3372, 'Skills'], [3322, 3337, 'Skills'], [3291, 3302, 'Skills'], [3262, 3271, 'Skills'], [3232, 3242, 'Skills'], [3203, 3209, 'Location'], [3074, 3080, 'Location'], [2927, 2933, 'Location'], [2920, 2926, 'Location'], [2854, 2860, 'Location'], [848, 852, 'Location'], [837, 845, 'Location'], [819, 835, 'Location'], [811, 817, 'Location'], [806, 809, 'Location'], [798, 804, 'Location'], [791, 797, 'Location'], [731, 737, 'Location'], [74, 80, 'Location'], [0, 15, 'Name']]},)\n",
      "Exception thrown when processing doc:\n(\"Ashish Dubey\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Ashish-Dubey/3e3e6614a65aeede\\n\\n❖ Maintaining Track record of conferring with customers by telephone or in person to provide\\ninformation about products or services. Expert in taking or entering orders, creating/canceling\\naccounts and obtaining details of complaints. In-depth knowledge of principles and processes of\\ncustomer needs assessment, meeting quality standards for services, and evaluation of customer\\nsatisfaction.\\n❖ Responsible and dependable Customer Service Representative with 4.6 years' experience\\nwith different companies. Highly skilled in performing office support duties such as providing\\ninformation to the public, receiving payments and addressing customer complaints. Hands on\\nexperience in ensuring that appropriate changes are made to resolve customers' problems and\\nreferring unresolved customer grievances to designated departments for further investigation.\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nRelationship Manager\\n\\nMatrimony.com -  Mumbai, Maharashtra\\n\\nTo handle VIP corporate customer profile , as a relationship manager for their marriage praposal .. \\nHandling all objection. \\nComplete sales target , \\nMeet with client , meeting schedule with prospect ,, able to work in all situation .\\n\\nExecutive (IT Sales)\\n\\nADITYA INFOTECH For Process Of CorelDraw Software as Compliance -\\n\\nJune 2016 to January 2017\\n\\nIdentifying potential areas of compliance vulnerability and risk; developing & implementing\\ncorrective action plans for resolution of problematic issue to the companies.\\n• Sourcing from internet & collecting information about the companies who are using Corel\\ndraw.\\n• Generating Leads as per targets daily.\\n• Thoroughly and efficiently gather company information, access and fulfill company needs.\\n• Educate the company about the compliance & importance of registration or\\nLicensing of Corel Draw.\\n\\n• Making them to purchase the license from the vendors if they are not still completed with the\\nlegalization procedure.\\n• Confirming about the purchase price of the license from the vendor's and providing the\\ncompanies the quotation.\\n• Follow-up with the companies whether they have completed with the legalization procedure or\\nthey have purchased the license.\\n\\nhttps://www.indeed.com/r/Ashish-Dubey/3e3e6614a65aeede?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Providing general guidance on Phone calls to the companies how to avoid or deal with similar\\nsituations in the future.\\n• Provides reports on a regular basis, and as directed or requested.\\n• Closing accounts of the leads which have been created.\\n• Completing the weekly, monthly targets that have been allotted.\\n\\nOperation Executive\\n\\nDuurbeen bespoke pvt ltd -  Delhi, Delhi -\\n\\nOctober 2013 to May 2016\\n\\nProfessionally handling incoming and incoming/outgoing calls of customers.\\n• Explaining them about the Best holiday destination and giving them relevant information about\\nthe product.\\n• Thoroughly and efficiently gather customer information, access and fulfill\\nCustomer needs, educate the customer where applicable to prevent the need for future contacts\\nand document interactions through contact tracking.\\n• Provide quality service and support in a variety of areas including, but not limited\\nto: billing, placing print orders, and system troubleshooting.\\n• Responsible for compiling and generating reports as they relate to customer\\nservice surveys.\\n• Ensure that issues are resolved both promptly and thoroughly if any.\\n\\nEDUCATION\\n\\nB.A\\n\\nBanaras Hindu University -  Varanasi, Uttar Pradesh\\n\\n2010\\n\\nSKILLS\\n\\nSALES (3 years), ACCESS (Less than 1 year), EXCEL (Less than 1 year), MICROSOFT OFFICE (3\\nyears), MS OFFICE (2 years)\\n\\nCERTIFICATIONS/LICENSES\\n\\nB1 certificate in french language\\n\\nMarch 2018\\n\\nADDITIONAL INFORMATION\\n\\n❖ Computer Proficiency: Basic knowledge of computer\\n(Microsoft Office Suite Word, Excel, Outlook, PPT, Access))\\n❖ Sales forces/ Air ticket generating on GDS software/\\n❖ Content /blog writing\\n❖ Social Media (face book, twitter, you tube, etc.)\\n\\n\\n\\nStrengths: Hard work, Perseverance, Team Skills, Patience.\\nIndian\\n\\nReligion: Hindu.\\n\\nAddress: L 16 street no.1 Mishra compound Opp- Sardar Patel school new link road Borivali (west)\\nMumbai 400068.\\n\\nDeclaration\\n\\nI hereby declare that above mentioned information is correct and true as per best of my\\nknowledge.\\n\\nDate:\\nPlace: (Ashish Chandra Dubey)\",) ({'entities': [[4228, 4234, 'Location'], [3683, 3692, 'Skills'], [3655, 3671, 'Skills'], [3628, 3634, 'Skills'], [3601, 3608, 'Skills'], [3585, 3590, 'Skills'], [3546, 3554, 'Location'], [1040, 1046, 'Location'], [1039, 1046, 'Location'], [13, 19, 'Location'], [1, 12, 'Name']]},)\n",
      "Exception thrown when processing doc:\n(\"Jitendra Razdan\\nBusiness Development - Shapoor ji Pallonji Group Company\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Jitendra-Razdan/66b1e69aff1842bd\\n\\n• Dynamic professional with more than 7 years of work experience in Sales and Customer\\nService to Banking, Government & Corporate Sector.\\n• Proficient in managing day to day sales activities, marketing and client servicing activities.\\n• Ability to develop and maintain relationship with key decision makers and resolving\\ncritical problems.\\n• Ability to think innovatively and grasp key concepts quickly..\\n• Effective communicator with an ability to deal effectively with various departments\\n\\nWORK EXPERIENCE\\n\\nBusiness Development\\n\\nShapoor ji Pallonji Group Company -  Mumbai, Maharashtra -\\n\\nJuly 2011 to Present\\n\\n2011.\\n\\n(Areas: Mumbai, Delhi, NCR, Punjab, Himachal Pradesh, Haryana and J&K)\\n\\nArea of Expertise & Exposure\\nKey Account Management:\\n• Interface with individuals / key influencers for ascertaining requirements, making Presentations\\nand delivering need based product solutions.\\n• Ensure speedy resolution of queries & grievances to maximize client satisfaction levels.\\n• Maintain excellent relations with customers to generate avenues for further business.\\n\\nBusiness Development:\\n• Analyze business potential, conceptualize & execute strategies to drive sales, augment\\nturnover and achieve desired targets.\\n• Monitor competitor activities and devise effective counter measures.\\n• Identify, explore and develop new markets and tap profitable business opportunities.\\n\\nKEY RESPONSIBILITIES\\n• Playing an integral role in new business pitches and hold responsibility for the effective on-\\nboarding of new clients.\\n• Responsible for the development and achievement of sales through the direct sales.\\n• Focusing on growing and developing existing clients, together with generating new business.\\n• Write business plans for all current and opportunity tender business.\\n• Act as the key interface between the customer and all relevant divisions.\\n• Preparation of technical proposals and participation in business development meetings with\\nexisting and new clients.\\n• Develop and expand client relationships with existing clients in order to generate repeat\\nbusiness.\\n\\nhttps://www.indeed.com/r/Jitendra-Razdan/66b1e69aff1842bd?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Negotiate projects with clients, issue proposals and quotations, follow-up quotations.\\n• Looks after customer satisfaction regarding operational issues.\\n• Giving sales presentations to high-level executives.\\n• Managing employer's client relationships and providing clients with excellent service and\\nsupport.\\n• Feeding back all suggestions for improvement and market research to senior staff.\\n• Market Research\\n• People and Performance Management\\n• Pre and Post Sales\\n\\nKey Accounts Manager\\n\\nForbes Technosys Limited -  Mumbai, Maharashtra\\n\\na\\n\\nSales Executive\\n\\nSUN SYSTEM PVT LIMITED -  Delhi, Delhi -\\n\\nJanuary 2011 to May 2011\\n\\nwith Sun System Pvt Limited, sole distributor for APPLE\\nproducts as well as computer related IT accessories, at Delhi\\nDuration: Jan 2011 to May 2011\\n\\nEDUCATION\\n\\nB.E in Electronics & Telecommunication\\n\\nS S JONDHALE College Of Engineering -  Mumbai, Maharashtra\\n\\nSKILLS\\n\\nCONFIDENT (Less than 1 year), MS OFFICE (Less than 1 year), PROACTIVE (Less than 1 year),\\nREPORTING TOOLS (Less than 1 year), SELF MOTIVATED (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nSKILLS/STRENGTHS:\\n\\n• Good Communication Skills\\n• Good Analytical and Research Skills\\n• Well organized\\n• Multi tasking\\n• Team player\\n• Determined\\n• Confident\\n• Innovative\\n• Proactive\\n\\n\\n\\nTECHNICAL ATTRIBUTES:\\n\\n• Operating Systems: Windows XP or higher\\n• Reporting Tools: Explorer, MS Office\",) ({'entities': [[3436, 3598, 'Skills'], [3358, 3372, 'Skills'], [3322, 3337, 'Skills'], [3291, 3302, 'Skills'], [3262, 3271, 'Skills'], [3232, 3242, 'Skills'], [3203, 3209, 'Location'], [3074, 3080, 'Location'], [2927, 2933, 'Location'], [2920, 2926, 'Location'], [2854, 2860, 'Location'], [848, 852, 'Location'], [837, 845, 'Location'], [819, 835, 'Location'], [811, 817, 'Location'], [806, 809, 'Location'], [798, 804, 'Location'], [791, 797, 'Location'], [731, 737, 'Location'], [74, 80, 'Location'], [0, 15, 'Name']]},)\n",
      "Exception thrown when processing doc:\n(\"Laxmiprasad Ukidawe\\nRegional Sales Manager - Sales & Business Development - A. Company\\n\\nThane, Maharashtra - Email me on Indeed: indeed.com/r/Laxmiprasad-\\nUkidawe/70461ec2893fd3c2\\n\\n➢ A result-driven MBA professional, with relevant experience in managing Key Accounts in the\\nMarine Industry.\\n➢ Currently working: AkzoNobel Coatings India Pvt Ltd (in Marine Coating sBU)\\n➢ Expertise in Sales & Technical Service\\n\\nWORK EXPERIENCE\\n\\nRegional Sales Manager - Sales & Business Development\\n\\nA. Company -\\n\\nOctober 2005 to Present\\n\\nJob Responsibilities\\n• Analysis of the existing customer with respect to sales and make a retention strategy in the\\ncompetitive market\\n• Strategy for each customer, analyze the needs and offer best solution\\n• Study the product / system which may be best fit for the ship/marine applications\\n• Study the requirement of customer, our own product and offer the system which can help to\\nreduce the operating cost and increase the bottom line\\n• Preparation of quotation, Tenders, Long term offers with respect to pricing for multiple products,\\nmulti countries, in multiple currencies for 24 or 36 months\\n• Budgeting as per the given guidelines\\n• Identify the potential customer, critically analyze the project, complete all the procedures for\\nthe vendor registration and also with the consultants / contractors\\n• Product pricing, formulating and implementing strategy long / short term with particular\\ncustomer\\n• Liaison with World wide technical office in the UK on regular basis for compliances with\\nclassification societies.\\n\\nSales Manager (Marine Paints)\\n\\nB. Company -\\n\\nJune 2002 to September 2005\\n\\nJob Responsibilities\\n• To assist the senior managers in setting new India office.\\n• Identify and analyze the business opportunities in India\\n• Responsibilities also included the brand and product awareness through extensive marketing\\ndrive with support from regional head office.\\n• Identify the product mix for the India region for organized and un-organized markets.\\n\\nArea Sales Manager- Marine Paints\\n\\nhttps://www.indeed.com/r/Laxmiprasad-Ukidawe/70461ec2893fd3c2?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Laxmiprasad-Ukidawe/70461ec2893fd3c2?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nChugoku Jenson & Nicholson Ltd -  Mumbai, Maharashtra -\\n\\nApril 2001 to June 2002\\n\\nJob Responsibilities\\n• Sales and technical service for Mumbai region.\\n• To achieve higher sales with major ship owners\\n• Identify new potential of shipyards.\\n• Single point contact for all sales and service requirements.\\n\\nTechnical Sales Executive- Marine& Protective Paints\\n\\nBombay Paints Ltd -  Mumbai, Maharashtra -\\n\\nNovember 1994 to April 2001\\n\\nJob Responsibilities\\n• Paint testing and paint formulations\\n• Paint trials at customer's sites and product developments\\n• Technical service in OEM & Marine industry (M&R)\\n\\nEDUCATION\\n\\nMarketing\\n\\nMumbai University -  Mumbai, Maharashtra\\n\\nB. Sc in Physics\\n\\nPune University -  Pune, Maharashtra\\n\\nADDITIONAL INFORMATION\\n\\nKey Skills: Sales & Marketing, Customer / Client relationship, Good Negotiation Skills, Team\\nLeadership and Management, Convincing skills, Good in Product knowledge.\",) ({'entities': [[2991, 3144, 'Skills'], [2877, 2884, 'Location'], [2610, 2617, 'Location'], [2368, 2375, 'Location'], [2265, 2272, 'Location'], [88, 106, 'Location'], [0, 19, 'Name']]},)\n",
      "Losses {'ner': 29864.81137348901}\nUnfiltered training data size:  503\nFiltered training data size:  498\nBad data size:  4\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from spacy_train_resume_ner import train_spacy_ner\n",
    "\n",
    "def remove_bad_data(training_data):\n",
    "    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n",
    "    ## training data is list of lists with each list containing a text and annotations\n",
    "    ## baddocs is a set of strings/resume texts.\n",
    "    ## TODO complete implementation to filter bad docs and store filter result (good docs) in filtered variable\n",
    "    filtered = [data for data in training_data if data[0] not in baddocs]\n",
    "    print(\"Unfiltered training data size: \",len(training_data))\n",
    "    print(\"Filtered training data size: \", len(filtered))\n",
    "    print(\"Bad data size: \", len(baddocs))\n",
    "    return filtered\n",
    "\n",
    "## call remove method. It may take a few minutes for the method to complete.\n",
    "## you will know it is complete when the print output above. \n",
    "X = remove_bad_data(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "498\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(len(X))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: How many bad docs did you have? What is the size of your new (filtered) training data? \n",
    "Five bad docs, 498 datasets in a new training data are present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Test Split\n",
    "Now before we train our model, we have to split our available training data into training and test sets. Splitting our data into train and test (or holdout) datasets is a fundamental technique in machine learning, and essential to avoid the problem of overfitting.\n",
    "Before we go on, you should get a grasp of how train/test split helps us avoid overfitting. Please take the time now to do a quick web search on the topic. There are many resources available. You should search for \"train test validation overfitting\" or some subset of those terms.\n",
    "\n",
    "Here are a few articles to start with:\n",
    "* https://machinelearningmastery.com/a-simple-intuition-for-overfitting/\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation (Note: you are free to install scikit learn and use the train_test_split method documented here, but it is not necessary. It is the concept that is important)\n",
    "\n",
    "##### Question: What is overfitting and how does doing a train/test split help us avoid overfitting when training our models? Please answer in your own words. \n",
    "An overfitting means the model contains more parameters to fit than the data provides. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand why we do a train/test split, we will write some code that splits our data into train and test sets. Usually we want around 70-80% of the data for train, and the rest for test. \n",
    "##### TODO: Complete the code below to create a train and test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Test:  150 , Train:  348\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##TODO complete the implementation  of the train test split function below\n",
    "def train_test_split(X,train_percent):\n",
    "    train_size = int(len(X) * (train_percent/100))\n",
    "    train = X[:train_size]\n",
    "    test = X[train_size:]\n",
    "    return train,test\n",
    "## TODO chose train size percent and call train test split, storing results in \"train\" and \"test\" variables.\n",
    "train,test = train_test_split(X, 70)\n",
    "## TODO use python assert to assert that the size of train and test sets add up to the size of all the data \n",
    "assert len(X) == len(train) + len(test)\n",
    "print(\"Test: \", len(test), \", Train: \", len(train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a spacy ner model with our training data\n",
    "OK, now it is (finally) time to train our own custom NER model using spacy. Because our training data has been preprocessed to only include annotations for the entities we are interested in, the model will only be able to predict/extract those entities. \n",
    "*Depending on your computer, this step may take a while.* Training 20 epochs (iterations) using 480 training examples takes around 10 minutes on my machine (core i7 CPU). You will see output like *Losses {'ner':2342.23342342}* after each epoch/iteration. The default number of iterations is 20, so you will see this output 20 times. When this step is done, we will use the trained ner model to perform predictions on our test data in our test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 37202.443002108776}\n",
      "Losses {'ner': 17735.210161813673}\n",
      "Losses {'ner': 13144.159044783013}\n",
      "Losses {'ner': 14216.834321991566}\n",
      "Losses {'ner': 12073.707825156314}\n",
      "Losses {'ner': 11121.387102987483}\n",
      "Losses {'ner': 12520.128805040964}\n",
      "Losses {'ner': 9805.358840240777}\n",
      "Losses {'ner': 7938.908639285404}\n",
      "Losses {'ner': 8074.862212184387}\n",
      "Losses {'ner': 7513.0699158959615}\n",
      "Losses {'ner': 7866.972667430955}\n",
      "Losses {'ner': 8478.064681466687}\n",
      "Losses {'ner': 7341.443052584882}\n",
      "Losses {'ner': 8162.654087951412}\n",
      "Losses {'ner': 7293.600492337902}\n",
      "Losses {'ner': 6607.747966766714}\n",
      "Losses {'ner': 7378.247041811197}\n",
      "Losses {'ner': 7095.02263463578}\n",
      "Losses {'ner': 6929.398206653635}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## run this code to train a ner model using spacy\n",
    "custom_nlp,_= train_spacy_ner(train,n_iter=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect NER predictions on one sample resume\n",
    "Now that we have a trained model, let's see how it works on one of our resumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## TODO fetch one resume out of our test dataset and store to the \"resume\" variable\n",
    "resume = test[42]\n",
    "## TODO create a spacy doc out of the resume using our trained model and save to the \"doc\" variable \n",
    "doc = custom_nlp(resume[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will output the predicted entities and the existing annotated entities in that doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "PREDICTED:\nName Harini Komaravelli\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nSkills Functional Testing, Blue Prism, Qtp\n\nADDITIONAL INFORMATION\n\nArea of Expertise:\n\n➢ Familiar with Agile Methodologies.\n➢ Having knowledge in Energy (Petroleum) & Health Care domains.\n➢ Involved in preparation of Test Scenarios.\n➢ Preparing Test Data for the test cases.\n\nhttps://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN\nhttps://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN\n\n\n➢ Experienced in development and execution of Test cases effectively.\n➢ Experienced in Functional testing, GUI testing, Smoke testing, Regression testing and\nIntegration Testing\n➢ Experienced in doing Accessibility testing of an application\n➢ Ability to understand user Requirements, Functional and Design specifications.\n➢ Good knowledge of SDLC and STLC processes.\n➢ Deciding the Severity and Priority of bugs.\n➢ Experience in using Microsoft Test Manager & Oracle Test Manager as Test Management Tools.\n➢ Having good experience in testing windows based & web based applications.\n➢ Involved in Client Interactions for reviews, issues and for any clarifications.\n➢ Web Services Testing\n➢ Writing Test Scripts in QTP, Testcomplete.\n➢ Creating Object Repositories and Function Libraries in QTP.\n➢ Enhanced QTP scripts using VB Script.\n➢ Strong experience in working with Blue Prism tool\n➢ Worked on different Environments like Windows Application & Web Application\n\nTechnical Skills:\n\n❑ Test Automation Tools: Blue Prism, QTP 10.0, Testcomplete\n❑ Test Management Tool: Microsoft Test Manager, Oracle Test Manager & JIRA\n❑ Databases: Oracle 10g, SQL Server.\n\n❑ Operating Systems: Windows 7\n\nProject 1:\nTitle: Cadence\nClient: Baker Hughes\n\nTechnologies: Microsoft Visual Studio and Microsoft Team Foundation Server\n\nClient Background:\nAn oilfield services company delivering focused efforts on shale gas and other oilfield services.\nIt provides services, tools and software for drilling and formation evaluation, well completion,\nproduction management, seismic data collection and interpretation.\n\nProject Description:\nAUT (Application under test) is the next generation revolutionary, robust, easy to use scalable\nwell site data acquisition processing and interpretation system for Client's Drilling Services to\ndeliver services that meets cross divisional business requirements consistently.\n\nProject 2:\n\nDescription:\nParagon supports your entire care team with one tool that your clinicians need to help deliver\nthe best patient care. Designed by physicians, nurses, pharmacists and mid level providers that\nhave a first-hand understanding of clinical workflow needs, Paragon clinical applications allow\nyour caregivers to focus on what matters most; spending time caring for patients. Since Paragon\nis fully-integrated across all applications and built around a single patient database, information\n\n\n\nentered anywhere in the system is immediately available to the entire care team. Immediate\naccess not only helps clinicians make better treatment decisions - it also helps promote patient\nsafety. Paragon offers a broad suite of multidisciplinary clinical software solutions together with\nanytime, anywhere access to the complete patient record.\n\nResponsibilities:\n\n• Performed Smoke testing and Regression testing.\n• Involved in Generating and Executing Test Script using Quick Test Pro & Blue Prism\n• Usability and User Interface Testing.\n• Involved in Defect tracking and reporting the bugs using TFS\n• Participated in frequent walk-through meetings with Internal Quality Assurance groups and with\ndevelopment groups.\n• Participated in client calls and clarifying the doubts by having AT&T sessions\n• Involved in functional, regression and smoke testing to validate the application data changes\ndone in windows application\n• Certifying the build status by running the scripts as part of smoke testing\n\nProject 3:\n\nDescription:\nFood & Beverages R&A: Easily manage business across multiple locations while reducing IT\ncost and complexity. Cloud-based point-of-sale (POS) solutions enable centralized enterprise\nmanagement with lower upfront costs and a smaller footprint.\n\nResponsibilities:\n\n• Performed Functional testing and Regression testing.\n• Involved in Generating and Executing Test Scripts using Blue Prism tool and Open script\n• Involved in preparing bots using Blue Prism tool.\n• Accessibility testing of the web application\n• Involved in Defect tracking and reporting the bugs using JIRA\n• WebServices testing by calling API's to export the data\nLABELED:\nSkills Functional Testing, Blue Prism, Qtp\n\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nLocation Hyderabad\nName Harini Komaravelli\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## TODO output label and text of predicted entities (in \"ents\" variable of the spacy doc created above)\n",
    "print(\"PREDICTED:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent)\n",
    "## TODO output labeled entities (in \"entities\" dictionary of resume)\n",
    "print(\"LABELED:\")\n",
    "for ent in resume[1][\"entities\"]:\n",
    "    print(\"{} {}\".format(ent[2], resume[0][ent[0]:ent[1]]))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for NER\n",
    "Now that we can predict entities using our trained model, we can compare our predictions with the original annotations in our training data to evaluate how well our model performs for our task. The original annotations have been annotated manually by human annotators, and represent a \"Gold Standard\" against which we can compare our predictions. \n",
    "\n",
    "For most classification tasks, the most common evaluation metrics are:\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* f1 score\n",
    "\n",
    "In order to understand these metrics, we need to understand the following concepts:\n",
    "* True positives - How many of the predicted entities are \"true\" according to the Gold Standard? (training annotation) \n",
    "* True negatives - How many entities did the model not predict which are actually not entities according to the Gold Standard?\n",
    "* False positives - How many entities did the model predict which are NOT entities according to the Gold Standard?  \n",
    "* False negatives - How many entities did the model \"miss\" - e.g. did not recognize as entities which are entities according to the Gold Standard? \n",
    "\n",
    "Before we go on, it is important that you understand true/false positives/negatives as well as the evaluation metrics above. Take some time now to research the web in order to find answers to the following questions:\n",
    "\n",
    "##### Question: How are the evaluation metrics above defined in the context of evaluating Machine Learning models? How do they relate to True/False Positives/Negatives above? Please provide an intuitive description as well as the mathmatical formula for each metric. \n",
    "    * precision = true_positive / (true_positive + false_positive)\n",
    "      Describes a factor of proper recognized entities to all recognized entities\n",
    "    * recall = true_positive / (true_positive + false_neagtive)\n",
    "      Describes a factor of proper recognized entities to all “real” entities\n",
    "    * F1 = 2 * (precision * recall)/(precision + recall)\n",
    "      Is a harmonic averege of precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Metrics based on token-level annotations or full entity-level. \n",
    "The concepts above are our first step toward understanding how to evaluate our model effectively. However, in NER, we need to take into account that we can calculate our metrics either based on all tokens (words) found in the document, or only on the entities found in the document.  \n",
    "\n",
    "##### Token-Level evaluation. \n",
    "Token level evaluation evaluates how accurately did the model tag *each individual word/token* in the input. In order to understand this, we need to understand something called the \"BILUO\" Scheme (or BILOU or BIO). The spacy docs have a good reference. Please read and familiarize yourself with BILUO. \n",
    "\n",
    "https://spacy.io/api/annotation#biluo\n",
    "\n",
    "Up to now, we have not been working with the BILUO scheme, but with \"offsets\" (for example: (112,150,\"Email\") - which says there is an \"Email\" entity between positions 112 and 150 in the text). We would like to be able to evaluate our models on a token-level using BILUO - so we need to convert our data to BILUO. Fortunately, Spacy provides a helper method to do this for us.\n",
    "\n",
    "*Execute the code below to see how our \"Gold Standard\" and predictions for our example doc above look in BILUO scheme.* \n",
    "Note: some of the lines might be ommited for display purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Tokens Predicted        True\n0                                             Shibin    B-Name      B-Name\n1                                         Raveendran    L-Name      L-Name\n2                                                            O           O\n3                                          TERRITORY         O           O\n4                                              SALES         O    U-Skills\n5                                            MANAGER         O           O\n6                                                  -         O           O\n7                                                KKR         O           O\n8                                              GROUP         O           O\n9                                                 OF         O           O\n10                                         COMPANIES         O           O\n11                                                           O           O\n12                                           Calicut         O  U-Location\n13                                                 ,         O           O\n14                                            Kerala         O           O\n15                                                 -         O           O\n16                                             Email         O           O\n17                                                me         O           O\n18                                                on         O           O\n19                                            Indeed         O           O\n20                                                 :         O           O\n21   indeed.com/r/Shibin-Raveendran/bc9e8962b30d7c1d         O           O\n22                                                           O           O\n23                                            Intend         O           O\n24                                                to         O           O\n25                                             build         O           O\n26                                                 a         O           O\n27                                            career         O           O\n28                                              with         O           O\n29                                           leading         O           O\n..                                               ...       ...         ...\n911                                             time         O           O\n912                                       management         O           O\n913                                           skills         O           O\n914                                              and         O           O\n915                                           strong         O           O\n916                                        attention         O           O\n917                                               to         O           O\n918                                           detail         O           O\n919                                                          O           O\n920                                         COMPUTER         O           O\n921                                      PROFICIENCY         O           O\n922                                                :         O           O\n923                                                          O           O\n924                                           COURSE         O           O\n925                                      INSTITUTION         O           O\n926                                             YEAR         O           O\n927                                                          O           O\n928                                            Tally         O           O\n929                                                ,         O           O\n930                                            Peach         O           O\n931                                             Tree         O           O\n932                                                ,         O           O\n933                                              DOA         O           O\n934                                             Sree         O           O\n935                                    Sankaracharya         O           O\n936                                                ,         O           O\n937                                           kannur         O           O\n938                                                [         O           O\n939                                                …         O           O\n940                                                ]         O           O\n\n[941 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Predicted</th>\n      <th>True</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Shibin</td>\n      <td>B-Name</td>\n      <td>B-Name</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Raveendran</td>\n      <td>L-Name</td>\n      <td>L-Name</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TERRITORY</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SALES</td>\n      <td>O</td>\n      <td>U-Skills</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MANAGER</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>KKR</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>GROUP</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>OF</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>COMPANIES</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Calicut</td>\n      <td>O</td>\n      <td>U-Location</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>,</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Kerala</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Email</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>me</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>on</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Indeed</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>:</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>indeed.com/r/Shibin-Raveendran/bc9e8962b30d7c1d</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Intend</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>to</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>build</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>a</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>career</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>with</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>leading</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>911</th>\n      <td>time</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>912</th>\n      <td>management</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>skills</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>and</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>strong</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>attention</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>to</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>detail</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>920</th>\n      <td>COMPUTER</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>921</th>\n      <td>PROFICIENCY</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>922</th>\n      <td>:</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>923</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>924</th>\n      <td>COURSE</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>925</th>\n      <td>INSTITUTION</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>926</th>\n      <td>YEAR</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>927</th>\n      <td></td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>928</th>\n      <td>Tally</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>929</th>\n      <td>,</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>930</th>\n      <td>Peach</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>931</th>\n      <td>Tree</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>932</th>\n      <td>,</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>933</th>\n      <td>DOA</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>934</th>\n      <td>Sree</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>935</th>\n      <td>Sankaracharya</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>936</th>\n      <td>,</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>937</th>\n      <td>kannur</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>938</th>\n      <td>[</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>939</th>\n      <td>…</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>940</th>\n      <td>]</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>941 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    \"\"\"\n",
    "    param nlp - a trained spacy model\n",
    "    param resume - a resume from our train or test set\n",
    "    \"\"\"\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc,\n",
    "                                                   [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df\n",
    "\n",
    "## TODO call method above with a resume from test set and store result in bilou_df variable.\n",
    "bilou_df = make_bilou_df(custom_nlp,test[0])\n",
    "display(bilou_df)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, it should be very easy to calculate a token-level accuracy. We simply compare the \"Predicted\" to \"True\" columns and calculate what percentage are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy on one resume:  0.9851222104144527\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## TODO bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
    "same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "## accuracy is the length of this subset divided by the length of bilou_df\n",
    "accuracy = same_df.shape[0] / bilou_df.shape[0]\n",
    "print(\"Accuracy on one resume: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy might seem pretty good... if it is not 100%, then let's print out those tokens where the model predicted something different than the gold standard by running the code below. \n",
    "\n",
    "Note - if your score on one doc is 100%, pick another document and re-run the last few cells above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Tokens   Predicted        True\n4        SALES           O    U-Skills\n12     Calicut           O  U-Location\n60       SALES           O    U-Skills\n214      SALES           O    U-Skills\n545     Mumbai  U-Location           O\n622    Calicut           O  U-Location\n724    Calicut           O  U-Location\n728    Calicut           O  U-Location\n750     RETAIL    U-Skills           -\n758     RETAIL    B-Skills           -\n759               L-Skills           O\n760  MARKETING    U-Skills           O\n768      SALES    B-Skills    U-Skills\n769      FORCE    L-Skills           O",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Predicted</th>\n      <th>True</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>SALES</td>\n      <td>O</td>\n      <td>U-Skills</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Calicut</td>\n      <td>O</td>\n      <td>U-Location</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>SALES</td>\n      <td>O</td>\n      <td>U-Skills</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>SALES</td>\n      <td>O</td>\n      <td>U-Skills</td>\n    </tr>\n    <tr>\n      <th>545</th>\n      <td>Mumbai</td>\n      <td>U-Location</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>Calicut</td>\n      <td>O</td>\n      <td>U-Location</td>\n    </tr>\n    <tr>\n      <th>724</th>\n      <td>Calicut</td>\n      <td>O</td>\n      <td>U-Location</td>\n    </tr>\n    <tr>\n      <th>728</th>\n      <td>Calicut</td>\n      <td>O</td>\n      <td>U-Location</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>RETAIL</td>\n      <td>U-Skills</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>758</th>\n      <td>RETAIL</td>\n      <td>B-Skills</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td></td>\n      <td>L-Skills</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>MARKETING</td>\n      <td>U-Skills</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>768</th>\n      <td>SALES</td>\n      <td>B-Skills</td>\n      <td>U-Skills</td>\n    </tr>\n    <tr>\n      <th>769</th>\n      <td>FORCE</td>\n      <td>L-Skills</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO find all rows in bilou_df where \"Predicted\" not equal to \"True\" column. \n",
    "diff_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "display(diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the accuracy on all our test resumes and average them for an accuracy score. \n",
    "\n",
    "Please complete the code below to report an accuracy score on our test resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy:  0.874387119850841\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_accuracy = []\n",
    "for res in test:\n",
    "    ## TODO calculate accurac for each 'res' and append to doc_accuracy list \n",
    "    res_df = make_bilou_df(custom_nlp, res)\n",
    "    same_df = res_df[res_df[\"Predicted\"] == res_df[\"True\"]]\n",
    "    accuracy = float(same_df.shape[0]) / res_df.shape[0]\n",
    "    doc_accuracy.append(accuracy)\n",
    "\n",
    "## TODO calculate mean/average of doc_accuracy (Tip: use numpy!)\n",
    "total_acc = np.mean(doc_accuracy)\n",
    "print(\"Accuracy: \",total_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how does the model perform on token-level accuracy? What did it miss? In those cases where the predictions didn't match the gold standard, were the predictions plausible or just \"spurious\" (wrong)? \n",
    "*Answer here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What might the advantages and disadvantages be of calculating accuracy on token-level? Hint: think about a document with 1000 tokens where only 10 tokens are annotated as entities. What might the accuracy be on such a document?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity-Level evaluation #####\n",
    "Another method of evaluating the performance of our NER model is to calculate metrics not on token-level, but on entity level. There is a good blog article that describes this method. \n",
    "\n",
    "http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "\n",
    "The article goes into some detail, the most important part is the scenarios described in the section \"Comparing NER system output and golden standard\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how do the first 3 scenarios described in the section \"Comparing NER system output and golden standard\" correlate to  true/false positives/negatives? \n",
    "*Answer here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision, Recall, F1 #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to calculate precision, recall, and f1 for each entity type we are interested in (our chosen entities). To do this, we need to understand the formulas for each. A good article for this is https://skymind.ai/wiki/accuracy-precision-recall-f1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how can we calculate precision, recall and f1 score based on the information above? Please provide the formulas for each #####\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supply code below which calculates precision and recall and F1 on our test data for each entity type we are interested in. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "For label 'Skills' tp: 41 fp: 12 fn: 2\n",
      "For label 'Name' tp: 2 fp: 0 fn: 0\n",
      "For label 'Location' tp: 1 fp: 1 fn: 2\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "          Precision    Recall        f1\nSkills     0.773585  0.953488  0.854167\nName       1.000000  1.000000  1.000000\nLocation   0.500000  0.333333  0.400000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Skills</th>\n      <td>0.773585</td>\n      <td>0.953488</td>\n      <td>0.854167</td>\n    </tr>\n    <tr>\n      <th>Name</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>Location</th>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
    "data = []\n",
    "for label in chosen_entity_labels:\n",
    "    ## variables to store results for all resumes for one entity type\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for tres in test:\n",
    "        ## use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives,\n",
    "        ## and false negatives. \n",
    "        tres_df = make_bilou_df(custom_nlp, tres)\n",
    "        ## calculate true false positives and false negatives for each resume\n",
    "        tp = tres_df[(tres_df[\"Predicted\"] == tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))]\n",
    "        fp = tres_df[(tres_df[\"Predicted\"] != tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))]\n",
    "        fn = tres_df[(tres_df[\"Predicted\"] != tres_df[\"True\"]) & (tres_df[\"True\"].str.contains(label))]\n",
    "        ## aggregate result for each resume to totals\n",
    "        true_positives = tp.shape[0]\n",
    "        false_positives = fp.shape[0]\n",
    "        false_negatives = fn.shape[0]\n",
    "    \n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "    \n",
    "    ## TODO Use the formulas you learned to calculate metrics and print them out\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    if true_positives:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives+false_negatives)\n",
    "\n",
    "    f1 = 0.0\n",
    "    if precision+recall:\n",
    "        f1 = 2*((precision * recall) / (precision + recall))\n",
    "    #print(\"Precision: \",precision)\n",
    "    #print(\"Recall: \",recall)\n",
    "    #print(\"F1: \",f1)\n",
    "    row = [precision,recall,f1]\n",
    "    data.append(row)\n",
    "\n",
    "## make pandas dataframe with metrics data. Use the chosen entity labels as an index, and the metric names as columns. \n",
    "metric_df = pd.DataFrame(data, index=chosen_entity_labels, columns=[\"Precision\", \"Recall\", \"f1\"])\n",
    "display(metric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute an average score for each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO compute average metrics And print them out. Use pandas dataframe \"mean\" method to do this\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: how do the average metrics here (computed on entity-level) compare to the token-level accuracy score above? Which metric(s) would you prefer to use to evaluate the quality of your model? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost Done with part II! We just need to save our BILUO training data for reuse in Part III. \n",
    "For part III we are using flair and loading our data from a .csv file into a flair \"Corpus\". This is described here:\n",
    "\n",
    "https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md#reading-your-own-sequence-labeling-dataset\n",
    "\n",
    "We need to create \"train\" and \"test\" .csv files using our train and test dataset which corresponds to the format described above. This format is one line containing (minimally) a text token and a NER Tag. These should be separated by whitespace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Sentences\" #####\n",
    "Flair works with \"Sentences\" which is a list of tokens. If we simply write out our csv with one line for every token in our dataset, we will have 1 giant sentence with many thousands of words.. This is not what we want. \n",
    "We would like to partition our data so that we have a list of \"Sentences\" - corresponding to our intuition for a sentence - a sequence of words that belong together and is not all to long, usually separated by some punctuation. \n",
    "When we create our .csv strings/files, we need to do so so that they represent a list of sentences, each sentence consisting of a list of tokens/tags (each token/tag being one line in our csv). \n",
    "\n",
    "***To do this, create a blank newline after each sentence.*** \n",
    "\n",
    "How you split your resume data into sentences is your decision however.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some starter code is provided below. Complete it to create two strings which you will save as .csv. Each file will be a (long) list of token/tag lines, with sentences separated by newlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilou_for_flair(nlp, train, test):\n",
    "    \"\"\"\n",
    "    make .csv strings from train and test for use in flair\n",
    "    \"\"\"    \n",
    "    print(\"Make bilou dfs\")\n",
    "    ## makes a list of pandas dataframes, one for each resume. \n",
    "    training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "    test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "    print(\"Done!\")\n",
    "    ## strings to return\n",
    "    training_file = \"\"\n",
    "    test_file = \"\"\n",
    "    for idx,df in enumerate(training_data_as_bilou):\n",
    "        ## TODO - remove unwanted whitespace and/or newline token rows from dataframe\n",
    "        ## TODO - insert newlines after each \"sentence\" \n",
    "        ## You can use df.to_csv below to create a csv for your data\n",
    "        ## as_csv = df.to_csv(None,sep=\" \",encoding=\"utf-8\",index=False,header=False,line_terminator=\"\\n\")\n",
    "        ##\n",
    "        ## training_file = TODO append csv string for each resume to big training file (dont forget newline after each)\n",
    "    for idx, df in enumerate(test_data_as_bilou):\n",
    "        ## TODO same as above but for testfile\n",
    "        pass\n",
    "        \n",
    "    return training_file,test_file\n",
    "\n",
    "training,test = bilou_for_flair(custom_nlp,train,test)\n",
    "## TODO save training and test as .csv (wherever you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data we persisted with flair before we go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from flair.datasets import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "# folder where training and test data are\n",
    "data_folder = None\n",
    "# your training file name\n",
    "train_file = None\n",
    "\n",
    "# your training file name\n",
    "test_file = None\n",
    "\n",
    "columns = ## TODO how is your file structured? {}\n",
    "\n",
    "## Now load our csv into flair corpus\n",
    "corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n",
    "                                               train_file=train_file,\n",
    "                                               test_file=test_file)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you could load the corpus without error, you are ready to go on to part 3, where we will work with flair nlp!\n",
    "\n",
    "If you have something like between 5 and 10k training sentences, and roughly a third of that as"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}